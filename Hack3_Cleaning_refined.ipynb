{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning refined and creating clean file v2\n",
    "* &\n",
    "* html syntax\n",
    "* map (+ brand trfn; keep text -> EDA ) (save)\n",
    "* clean (save)\n",
    "* stem (save)\n",
    "* lemma (save)\n",
    "* gingerit (save)\n",
    "* gingerit then lemma (save)\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Mappings - 'iPhone 4/4s' -> 'iPhone4/4s' 'iPad 2' -> 'iPad2' so that we don't find stray charachters\n",
    "text emojis replaced with their word synonyms (<3 converted to 'love')\n",
    "HTML characters (eg: &lt; &gt; &quot;) and syntax (eg: <title>) removed\n",
    "HTML character'&amp;' replaced with '&' and brand names like AT&T and GSD&M were retained\n",
    "Links (http), short-links, date, time, non-ascii characters were removed\n",
    "'d 've 'll etc. replaced by their extended forms: would, have, will respectively (eg: 'they'll' converted to 'they will')\n",
    "Substituted links and mentions removed\n",
    "Hash symbol was removed, not the entire hash-tag\n",
    "Stemming applied.\n",
    "Lemmatization tried*\n",
    "GingerIt library used to correct spellings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.tools as tls   \n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingerit.gingerit import GingerIt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Aniket/MyStuff/Study/GreyAtom/Hackathon#3/Hack3_gen_functions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Hackathon3GeneralFunctions as genf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_iphone4(text):\n",
    "    iphone4_pat = r\"iphone[ ]+4\"\n",
    "    return re.sub(iphone4_pat, 'iPhone4', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ipads(text):\n",
    "    #1\n",
    "    ipads_pat = r\"ipads\"\n",
    "    return re.sub(ipads_pat, 'ipad', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ipad2(text):\n",
    "    #2\n",
    "    ipad2_pat = r\"ipad[ ]+2[s]?\"\n",
    "    return re.sub(ipad2_pat, 'ipad2', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_brand(text):\n",
    "    #EDA\n",
    "    iphone_pat = r\"iphone\"\n",
    "    ipad_pat = r\"ipad\"\n",
    "    apple_pat = r\"apple\"\n",
    "    temp = text\n",
    "    temp = re.sub(iphone_pat, 'iPhone', temp, flags=re.I)\n",
    "    temp = re.sub(ipad_pat, 'iPad', temp, flags=re.I)\n",
    "    temp = re.sub(apple_pat, 'Apple', temp, flags=re.I)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_amp(text):\n",
    "    pat_amp = r\"&amp;\"\n",
    "    return re.sub(pat_amp, '&', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_amp(text):\n",
    "    pat_amp_single = r\" & \"\n",
    "    return re.sub(pat_amp_single, '', text, flags=re.I)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_amp_op(text):\n",
    "    pat_amp_single = r\" & |& | &\"\n",
    "    return re.sub(pat_amp_single, '', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    return lemma(lnlp, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_syn(text):    \n",
    "#     html_syn_pat = r\"&lt;[a-z]+&gt;\"\n",
    "    html_syntax_pat = r\"&lt;[/a-z]+&gt;\"\n",
    "    return re.sub(html_syntax_pat, ' ', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text_emoji(text):\n",
    "    love_pat = r\"&lt;3+\"\n",
    "    happy_pat = r\":[-]?[\\)]+\"\n",
    "    sad_pat = r\":[-]?[\\(]+\"\n",
    "    playful_pat = r\":[-]?[p]+\"\n",
    "    wink_pat = r\";[-]?[\\)]+\"\n",
    "    straightface_pat = r\":[-]?[\\|]+\"\n",
    "    \n",
    "    #replace text emojis\n",
    "    temp = text\n",
    "    temp =  re.sub(love_pat, ' love ', temp)\n",
    "    temp =  re.sub(happy_pat, ' smiley ', temp)\n",
    "    temp =  re.sub(sad_pat, ' sad ', temp)\n",
    "    temp =  re.sub(playful_pat, ' playful ', temp, flags=re.I)\n",
    "    temp =  re.sub(wink_pat, ' wink ', temp)\n",
    "    temp =  re.sub(straightface_pat, ' straightface ', temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_before_cleaning(text):\n",
    "    temp = text\n",
    "    temp = map_iphone4(temp)\n",
    "    temp = map_ipads(temp)\n",
    "    temp = map_ipad2(temp)\n",
    "    temp = remove_html_syn(temp)\n",
    "    temp = replace_amp(temp)\n",
    "    temp = remove_amp(temp)\n",
    "    temp = replace_text_emoji(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = GingerIt()\n",
    "# line= \"I loooveee Apple's new iPad ipad 2 with AT&T/GSM&D deal #ipad2\"\n",
    "# tweet=parser.parse(line)\n",
    "# tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on -PRON- foot for good'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on -PRON- foot for good # apple Apple'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(\"The striped bats are hanging on their feet for best #apple Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a title '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_syn(\"&lt;title&gt;This is a title&lt;/title&gt;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' love  ipad2 iPhone4s &lt;   #ipad2'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_before_cleaning(\"&lt;3 ipad 2 iphone 4s &lt; &lt;title&gt; #ipad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', encoding='utf-8')\n",
    "data = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[0:10,'tweet'].map(refine_before_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_cleaned'] = data['tweet'].map(refine_before_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_cleaned'] = data['tweet_cleaned'].map(genf.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #sxswnui #sxsw #apple defining language of tou...\n",
       "1    Learning ab Google doodles! All doodles should...\n",
       "2    one of the most in-your-face ex. of stealing t...\n",
       "3    This iPhone #SXSW app would b pretty awesome i...\n",
       "4    Line outside the Apple store in Austin waiting...\n",
       "Name: tweet_cleaned, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet_cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_cleaned'] = data['tweet_cleaned'].map(remove_amp_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     sxswnui  sxsw  apple defining language of tou...\n",
       "1    learning ab google doodles  all doodles should...\n",
       "2    one of the most in your face ex  of stealing t...\n",
       "3    this iphone  sxsw app would b pretty awesome i...\n",
       "4    line outside the apple store in austin waiting...\n",
       "Name: tweet_cleaned, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet_cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_raw = ' '.join(data['tweet'].astype(str))\n",
    "sent_clean = ' '.join(data['tweet_cleaned'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(\".\"*8+\"iphone[4]?\"+\".\"*8,sent_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_stem'] = data['tweet_cleaned'].map(genf.tokenize_stem_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sxswnui sxsw appl defin languag of touch with ...\n",
       "1    learn ab googl doodl all doodl should be light...\n",
       "2    one of the most in your face ex of steal the s...\n",
       "3    thi iphon sxsw app would b pretti awesom if it...\n",
       "4    line outsid the appl store in austin wait for ...\n",
       "Name: tweet_stem, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet_stem'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tweet_lem'] = data['tweet_cleaned'].map(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tweet_lem'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tw id 5025 and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    59.265878\n",
       "2    32.746769\n",
       "0     6.268903\n",
       "3     1.718449\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/train_cleaned_v2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wo3 = data[data.sentiment != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    60.302140\n",
       "2    33.319345\n",
       "0     6.378514\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wo3.sentiment.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wo3.to_csv('data/train_cleaned_v2_wo3.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1819, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv', encoding='utf-8')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet'] = test['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet_cleaned'] = test['tweet'].map(refine_before_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet_cleaned'] = test['tweet_cleaned'].map(genf.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    audience q  what prototyping tools do you use ...\n",
       "1    at sxsw  send your best photosvideos to       ...\n",
       "2       and here  a pic of you winning your ipad   ...\n",
       "3    google marissa mayer  mobile phone as a cursor...\n",
       "4       sxsw google maps is even cooler than i thought\n",
       "Name: tweet_cleaned, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tweet_cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet_cleaned'] = test['tweet_cleaned'].map(remove_amp_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    audience q  what prototyping tools do you use ...\n",
       "1    at sxsw  send your best photosvideos to       ...\n",
       "2       and here  a pic of you winning your ipad   ...\n",
       "3    google marissa mayer  mobile phone as a cursor...\n",
       "4       sxsw google maps is even cooler than i thought\n",
       "Name: tweet_cleaned, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tweet_cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_raw = ' '.join(test['tweet'].astype(str))\n",
    "test_sent_clean = ' '.join(test['tweet_cleaned'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(\".\"*8+\"iphone[4]?\"+\".\"*8,sent_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet_stem'] = test['tweet_cleaned'].map(genf.tokenize_stem_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('data/test_cleaned_v2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
