{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "* use lemma\n",
    "* use pos tagger\n",
    "* sentiment analysis\n",
    "---\n",
    "* clean slangs\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import plotly\n",
    "# import plotly.tools as tls   \n",
    "# import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/Aniket/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Aniket/MyStuff/Study/GreyAtom/Hackathon#3/Hack3_gen_functions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Hackathon3GeneralFunctions as genf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7274 entries, 0 to 7273\n",
      "Data columns (total 3 columns):\n",
      "tweet_id     7274 non-null int64\n",
      "tweet        7273 non-null object\n",
      "sentiment    7274 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7274, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop null\n",
    "data.drop(1274,axis=0,inplace=True)\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7273, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweet'] = data.tweet.map(lambda x: genf.clean_text(x,lower=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textblob test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags for sent 1\n",
      "[('sxswnui', 'NN'), ('sxsw', 'NN'), ('apple', 'NN'), ('defining', 'VBG'), ('language', 'NN'), ('of', 'IN'), ('touch', 'JJ'), ('with', 'IN'), ('different', 'JJ'), ('dialects', 'NNS'), ('becoming', 'VBG'), ('smaller', 'JJR')]\n",
      "---\n",
      "Nounphrases for sent\n",
      "['sxswnui sxsw apple', 'different dialects']\n",
      "===\n",
      "\n",
      "POS tags for sent 2\n",
      "[('Learning', 'VBG'), ('ab', 'JJ'), ('Google', 'NNP'), ('doodles', 'VBZ'), ('All', 'DT'), ('doodles', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('light', 'JJ'), ('funny', 'JJ'), ('innovative', 'JJ'), ('with', 'IN'), ('exceptions', 'NNS'), ('for', 'IN'), ('significant', 'JJ'), ('occasions', 'NNS'), ('GoogleDoodle', 'NNP'), ('sxsw', 'NN')]\n",
      "---\n",
      "Nounphrases for sent\n",
      "['learning', 'google', 'light funny innovative', 'significant occasions', 'googledoodle']\n",
      "===\n",
      "\n",
      "POS tags for sent 3\n",
      "[('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('in', 'IN'), ('your', 'PRP$'), ('face', 'NN'), ('ex', 'NN'), ('of', 'IN'), ('stealing', 'VBG'), ('the', 'DT'), ('show', 'NN'), ('in', 'IN'), ('yrs', 'JJ'), ('RT', 'NNP'), ('At', 'IN'), ('SXSW', 'NNP'), ('Apple', 'NNP'), ('schools', 'NNS'), ('the', 'DT'), ('mkt', 'NN'), ('experts', 'NNS')]\n",
      "---\n",
      "Nounphrases for sent\n",
      "['face ex', 'rt', 'sxsw apple', 'mkt experts']\n",
      "===\n",
      "\n",
      "POS tags for sent 4\n",
      "[('This', 'DT'), ('iPhone', 'NN'), ('SXSW', 'NNP'), ('app', 'NN'), ('would', 'MD'), ('b', 'VB'), ('pretty', 'RB'), ('awesome', 'JJ'), ('if', 'IN'), ('it', 'PRP'), ('didnt', 'VBZ'), ('crash', 'NN'), ('every', 'DT'), ('10mins', 'CD'), ('during', 'IN'), ('extended', 'VBN'), ('browsing', 'NN'), ('Fuckit', 'NNP'), ('Illmakeitwork', 'NNP')]\n",
      "---\n",
      "Nounphrases for sent\n",
      "['sxsw', 'pretty awesome', 'didnt crash', 'fuckit illmakeitwork']\n",
      "===\n",
      "\n",
      "POS tags for sent 5\n",
      "[('Line', 'NNP'), ('outside', 'IN'), ('the', 'DT'), ('Apple', 'NNP'), ('store', 'NN'), ('in', 'IN'), ('Austin', 'NNP'), ('waiting', 'VBG'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('iPad', 'NN'), ('SXSW', 'NNP')]\n",
      "---\n",
      "Nounphrases for sent\n",
      "['apple', 'austin', 'new ipad', 'sxsw']\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    wiki = TextBlob(data.loc[i,'clean_tweet'])\n",
    "    print(f'POS tags for sent {i+1}')\n",
    "    print(wiki.tags)\n",
    "    print('---')\n",
    "    print(f'Nounphrases for sent')\n",
    "    print(wiki.noun_phrases)\n",
    "    print('===')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 1:  sxswnui  sxsw  apple defining language of touch with different dialects becoming smaller\n",
      "Sentiment for sent 1: Sentiment(polarity=0.15, subjectivity=0.65)\n",
      "Actual sentiment: 1\n",
      "===\n",
      "\n",
      "Sent 2: Learning ab Google doodles  All doodles should be light  funny   innovative  with exceptions for significant occasions   GoogleDoodle  sxsw\n",
      "Sentiment for sent 2: Sentiment(polarity=0.38125, subjectivity=0.89375)\n",
      "Actual sentiment: 1\n",
      "===\n",
      "\n",
      "Sent 3: one of the most in your face ex  of stealing the show in yrs RT    At  SXSW  Apple schools the mkt experts    \n",
      "Sentiment for sent 3: Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "Actual sentiment: 2\n",
      "===\n",
      "\n",
      "Sent 4: This iPhone  SXSW app would b pretty awesome if it didnt crash every 10mins during extended browsing   Fuckit  Illmakeitwork\n",
      "Sentiment for sent 4: Sentiment(polarity=0.625, subjectivity=1.0)\n",
      "Actual sentiment: 0\n",
      "===\n",
      "\n",
      "Sent 5: Line outside the Apple store in Austin waiting for the new iPad  SXSW   \n",
      "Sentiment for sent 5: Sentiment(polarity=0.06818181818181818, subjectivity=0.25227272727272726)\n",
      "Actual sentiment: 1\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    wiki = TextBlob(data.loc[i,'clean_tweet'])\n",
    "    print(f\"Sent {i+1}:\",wiki)\n",
    "    print(f'Sentiment for sent {i+1}: {wiki.sentiment}')\n",
    "    print(f\"Actual sentiment: {data.loc[i,'sentiment']}\")\n",
    "    print('===')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"His is a sample! I looooveee shone pad in\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = TextBlob(\"This is a sampleee! I looooveee iPhone ipad iOS\")\n",
    "w.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob(\"The new iOS is good. Good job Apple apples\").tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_slang_loopup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d31d3f59e106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_slang_loopup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_slang_loopup' is not defined"
     ]
    }
   ],
   "source": [
    "_slang_loopup(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"I loooveee Apple's new iPad ipad 2 with AT&T/GSM&D deal #ipad2 having\",\n",
       " 'result': \"I love Apple's new iPad iPad 2 with AT&T/GSM&D deal #ipad2 having\",\n",
       " 'corrections': [{'start': 28,\n",
       "   'text': 'ipad',\n",
       "   'correct': 'iPad',\n",
       "   'definition': None},\n",
       "  {'start': 2,\n",
       "   'text': 'loooveee',\n",
       "   'correct': 'love',\n",
       "   'definition': 'have a great affection or liking for'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "parser = GingerIt()\n",
    "line= \"I loooveee Apple's new iPad ipad 2 with AT&T/GSM&D deal #ipad2 having\"\n",
    "tweet=parser.parse(line)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love Apple's new iPad iPad 2 with AT&T/GSM&D deal #ipad2 having\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
