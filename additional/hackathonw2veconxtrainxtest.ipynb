{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "train = pd.read_csv('../data/train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        1\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ItemID\n",
    "\n",
    "#train.drop('ItemID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       #sxswnui #sxsw #apple defining language of tou...\n",
       "1       Learning ab Google doodles! All doodles should...\n",
       "2       one of the most in-your-face ex. of stealing t...\n",
       "3       This iPhone #SXSW app would b pretty awesome i...\n",
       "4       Line outside the Apple store in Austin waiting...\n",
       "5       #technews One lone dude awaits iPad 2 at Apple...\n",
       "6       SXSW Tips, Prince, NPR Videos, Toy Shopping Wi...\n",
       "7       NU user RT @mention New #UberSocial for #iPhon...\n",
       "8          Free #SXSW sampler on iTunes {link} #FreeMusic\n",
       "9       I think I might go all weekend without seeing ...\n",
       "10      RT @mention Official #SXSW App Û÷SXSW GOÛª b...\n",
       "11      It's official! I'm buying an iPad. #SXSW #elevate\n",
       "12      They're giving away iPad 2's, x boxes and book...\n",
       "13      RT @mention We're officially at #SXSW! Come by...\n",
       "14      #Companies to watch, from the #SXSW trade show...\n",
       "15      RT @mention Google Marissa Mayer, future of lo...\n",
       "16      DL the #Calyp App to get into #Calyp Casa at #...\n",
       "17      Well yeah.  Music &gt; iPhone nerds RT @mentio...\n",
       "18      Apple Opens Pop Up Store at SXSW {link} via @m...\n",
       "19      @mention atleast you are at #sxsw. I'm not the...\n",
       "20      Cue the choir music #SXSW  @mention Apple Stor...\n",
       "21      Anyone at #sxsw want an iPad 2? I'm in line an...\n",
       "22      _¼ÛÄ___ü ___¡  _____«_µ... &gt;&gt; @mention...\n",
       "23      RT @mention P.S. @mention and Google throw a b...\n",
       "24      .@mention I have a 3G iPhone. After 3 hrs twee...\n",
       "25      does anyone know if google did talk about #cir...\n",
       "26      ÛÏ@mention Google set to launch new social ne...\n",
       "27      Google to Launch Major New Social Network Call...\n",
       "28      standing on a long line surrounded by unemploy...\n",
       "29      WOW! *Something Ventured* was a kick ass film....\n",
       "                              ...                        \n",
       "7244    Google looks to the future with mobile, locati...\n",
       "7245    Fantastico! RT @mention Marissa Mayer: Google ...\n",
       "7246    RT @mention \\r\\nAn Apple pop-uitp store at #SX...\n",
       "7247    wishes he could be at the @mention / @mention ...\n",
       "7248    Tomorrow, Charles Chen will be speaking about ...\n",
       "7249    Z7: Lead Don't Follow {link} [codes valid: 4:0...\n",
       "7250    RT @mention Apple sets up 5,000-square-foot te...\n",
       "7251    RT @mention #SXSW gear bag: iPad 2, iPhone, Mo...\n",
       "7252    Û÷Viagra for your communicationsÛª @mention ...\n",
       "7253    Interesting! RT @mention Google to Launch Majo...\n",
       "7254    Google to Launch Major New Social Network Call...\n",
       "7255    Sweet, Apple's opening a pop-up shop in the Sc...\n",
       "7256    RT @mention the future is about networks, not ...\n",
       "7257    Did you miss Google's VP of Search, Marissa Ma...\n",
       "7258    @mention massive lines at #sxsw apple store......\n",
       "7259    Exploring the World in 3D with XML combines Go...\n",
       "7260    RT @mention iPad hipster #AustinCrowd #SXSW {l...\n",
       "7261    The new Whrrl app is now live in the iPhone ap...\n",
       "7262    Come see something new about Google SketchUp P...\n",
       "7263    There are two apple stores in ATX!! RT @mentio...\n",
       "7264    Heading over to ballroom b 'your mom has an ip...\n",
       "7265    At #SXSW, Apple schools the marketing experts ...\n",
       "7266    @mention great stuff on Fri #SXSW: Marissa May...\n",
       "7267    The session #designingforkids is changing my m...\n",
       "7268    Great visualisation of the ghost movement logi...\n",
       "7269    @mention Google plze Tammi.  I'm in middle of ...\n",
       "7270    RT @mention ÷¼ Are you all set? ÷_ {link} ÷...\n",
       "7271    RT @mention Aha! Found proof of lactation room...\n",
       "7272    We just launched our iPad app at #SXSW! Get al...\n",
       "7273    The next fin serv battle is vs Apple, GOOG, Mo...\n",
       "Name: tweet, Length: 7273, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiments1 = '\\n'.join(train['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentiments1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hash(text):\n",
    "    hash_pat = r'#'\n",
    "    return re.sub(hash_pat, ' ', text)\n",
    "def remove_mention(text):\n",
    "    mention_pat = r'@mention|@[a-z]+'\n",
    "    return re.sub(mention_pat, ' ', text, flags=re.I)\n",
    "def remove_short_link(text):\n",
    "    short_link_pat = r\"bit\\.ly/[a-z0-9/\\-:\\.=%;,\\+\\*())&\\$!@\\[\\]#\\?~_\\.']*\"\n",
    "    return re.sub(short_link_pat, ' ', text, flags=re.I) \n",
    "def remove_http_link(text):\n",
    "    link_permit = r\"[a-z0-9/\\-:\\.=%;,\\+\\*())&\\$!@\\[\\]#\\?~_\\.']\"\n",
    "    http_link_pat = r\"http[s]?://\"+link_permit+\"+|//\"+link_permit+\"+|[\\w\\.]+\\.[a-z]+/\"+link_permit+\"+\" \n",
    "    return re.sub(http_link_pat, ' ', text, flags=re.I)\n",
    "def remove_sub_link(text):\n",
    "    link_pat = r'{link}'\n",
    "    return re.sub(link_pat, ' ', text, flags=re.I)\n",
    "def remove_html_char(text):\n",
    "    html_char_pat = r'&[a-z]+;'\n",
    "    return re.sub(html_char_pat, ' ', text, flags=re.I)    \n",
    "def remove_date(text):\n",
    "    pipe = r'|'\n",
    "    date_pat_mon = str()\n",
    "    months =   [r'January',\n",
    "                r'February',\n",
    "                r'March',\n",
    "                r'April',\n",
    "                r'May',\n",
    "                r'June',\n",
    "                r'July',\n",
    "                r'August',\n",
    "                r'September',\n",
    "                r'October',\n",
    "                r'November',\n",
    "                r'December']\n",
    "    for month in months:\n",
    "        date_pat_mon = date_pat_mon + month+r' \\d\\d, \\d\\d\\d\\d|'\n",
    "        date_pat_mon = date_pat_mon + month[:3]+r' \\d\\d, \\d\\d\\d\\d|'\n",
    "    date_pat_mon = date_pat_mon[:-1]\n",
    "#     date_pat_mon\n",
    "    date_pat =  r'\\d\\d/\\d\\d/\\d\\d\\d\\d|\\d\\d/\\d\\d/\\d\\d' + pipe + \\\n",
    "                r'\\d\\d\\.\\d\\d\\.\\d\\d\\d\\d|\\d\\d\\.\\d\\d\\.\\d\\d' + pipe + \\\n",
    "                r'\\d\\d-\\d\\d-\\d\\d\\d\\d|\\d\\d-\\d\\d-\\d\\d' + pipe + \\\n",
    "                r'{}'.format(date_pat_mon)\n",
    "    date_pat = r'{}'.format(date_pat)\n",
    "    return re.sub(date_pat, ' ', text, flags=re.I)\n",
    "def remove_short_date(text):\n",
    "    pipe = r'|'\n",
    "    short_date_pat = r'[\\d]?\\d/\\d\\d[\\d\\d]?' + pipe + r'[\\d]?\\d\\.\\d\\d[\\d\\d]?'\n",
    "    short_date_pat = r'{}'.format(short_date_pat)\n",
    "    return re.sub(short_date_pat, ' ', text)\n",
    "def remove_time(text):\n",
    "    pipe = r'|'\n",
    "    time_pat =  r'\\d\\d:\\d\\d:\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d[ ]?am'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d[ ]?am'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d[ ]?am'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d'\n",
    "    time_pat= r'{}'.format(time_pat)\n",
    "    return re.sub(time_pat,' ', text, flags=re.I)\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    sxsw_pat = r\"sxsw[a-z]*\"\n",
    "    return re.sub(sxsw_pat, ' ', text, flags=re.I) \n",
    "\n",
    "def remove_austin_texas(text):\n",
    "    austin_pat = r\"austin\"\n",
    "    texas_pat = r\"texas|tx\"\n",
    "    temp = text\n",
    "    temp = re.sub(austin_pat, ' ', temp, flags=re.I) \n",
    "    temp = re.sub(texas_pat, ' ', temp, flags=re.I) \n",
    "    return temp\n",
    "def remove_punctuation(text):\n",
    "    punctuation_pat_s = r'\\'s'\n",
    "    punctuation_pat_t = r'\\'t'\n",
    "    punctuation_pat_d = r'\\'d'\n",
    "    punctuation_pat_ve = r'\\'ve'\n",
    "    punctuation_pat_ll = r'\\'ll'\n",
    "    temp = text\n",
    "    temp = re.sub(punctuation_pat_s, ' ', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_t, 't', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_d, ' would', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_ve, ' have', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_ll, ' will', temp, flags=re.I)\n",
    "    return temp\n",
    "def remove_rt(text):\n",
    "    rt_pat = r'RT'\n",
    "    return re.sub(rt_pat, ' ', text)\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return re.sub(emoji_pattern,'', text)\n",
    "def remove_not_alnum(text):\n",
    "    \"\"\"\n",
    "    remove everything that is not alpha numeric\n",
    "    \"\"\"\n",
    "    only_text = r'[^a-z0-9 ]'\n",
    "    return re.sub(only_text, ' ', text, flags=re.I)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #replace links witn {link}\n",
    "    text = remove_short_link(text)\n",
    "    text = remove_http_link(text)\n",
    "    #remove {link}\n",
    "    text = remove_sub_link(text)\n",
    "    #hash\n",
    "    text = remove_hash(text)\n",
    "    #mention\n",
    "    text = remove_mention(text)\n",
    "    #rt\n",
    "    text = remove_rt(text)\n",
    "    #html spl chars\n",
    "    text = remove_html_char(text)\n",
    "    #time\n",
    "    text = remove_time(text)\n",
    "    #svxm\n",
    "    text=remove_sxsw(text)\n",
    "    #texas\n",
    "    text=remove_austin_texas(text)\n",
    "    #date\n",
    "    text = remove_date(text)\n",
    "    text = remove_short_date(text)\n",
    "    #punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    #emotjis\n",
    "    text = remove_emojis(text)\n",
    "    #remove spl chars\n",
    "    text = remove_not_alnum(text)\n",
    "    #return lower\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# def performCleansingForSentimentalAnalysisWithStopWords(completeText):\n",
    "# #     print('##### Get Tokens in Lower Case #######')\n",
    "# #     #Get tokens in lower case\n",
    "# #     lowercasetokens = getUniqueTokens(completeText.lower())\n",
    "# #     unique_lowercase_tokens = len(set(lowercasetokens))\n",
    "# #     print('Lowercase Unique Tokens are ' + format(unique_lowercase_tokens))\n",
    "# #     print('##### Applying  stemming #######')\n",
    "# #     #Applying the stemming\n",
    "# #     unique_lowercase_tokens_stemmed= stemData(lowercasetokens)\n",
    "# #     len_unique_lowercase_tokens_stemmed = len(unique_lowercase_tokens_stemmed)\n",
    "# #     print('Lowercase Unique Tokens after steming  ' + format(len_unique_lowercase_tokens_stemmed))\n",
    "# #     print('##### Applying stop words + removing Hashtags + mentions + Links + Short Links +  #######')\n",
    "#  #   hashtags = find_hashtags(lowercompleteText)\n",
    "#  #    mentions = find_mentions(lowercompleteText)\n",
    "#  #   links = find_links(lowercompleteText)\n",
    "# #    numbers = find_numbers(lowercompleteText)\n",
    "# #    emojis = find_emojis(lowercompleteText)\n",
    "# #    punctuations = list(punctuation)\n",
    "# #     unique_lowercase_tokens_stemmed_without_stopwords = removeCustomStopWords(lowercompleteText,hashtags,mentions,links,numbers,emojis,punctuations)\n",
    "# #     return unique_lowercase_tokens_stemmed_without_stopwords\n",
    "#     lowercompleteText = completeText.lower()\n",
    "#     import re\n",
    "#     hastag_pat = r'#[0-9a-z_]+'\n",
    "#     mentions_pat = r'\\@[0-9a-z]+'\n",
    "#     number_pat = r'-?\\d+\\.?\\d+|-?\\d+'\n",
    "#     links_pat= r'http://\\S+|https://\\S+'\n",
    "#     punctuations_pat= r'[.!?\\\\-]'\n",
    "#     #short_links_pat =\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     def remove_hashtags(tweet):\n",
    "#         return re.sub(hastag_pat, '', tweet)\n",
    "#     def remove_mentions(tweet):\n",
    "#         return re.sub(mentions_pat, '', tweet)\n",
    "#     def remove_numbers(tweet):\n",
    "#         return re.sub(number_pat, '',tweet)\n",
    "#     def remove_links(tweet):\n",
    "#         return re.sub(links_pat, '', tweet)\n",
    "#     def remove_emojis(tweet):\n",
    "#         return re.sub(emoji_pattern, '', tweet)\n",
    "#     def remove_shortLinks(tweet):\n",
    "#         return re.sub(short_links_pat, '', tweet)\n",
    "#     def remove_punctuations(tweet):\n",
    "#         return re.sub(punctuations_pat, '', tweet)\n",
    "#     def find_hashtags(tweet):\n",
    "#         return re.findall(hastag_pat, tweet)\n",
    "#     def find_mentions(tweet):\n",
    "#         return re.findall(mentions_pat, tweet, flags=re.I)\n",
    "#     def find_links(tweet):\n",
    "#         return re.findall(links_pat, tweet, flags=re.I)\n",
    "#     def find_numbers(tweet):\n",
    "#         return re.findall(number_pat, tweet)\n",
    "#     def find_emojis(tweet):\n",
    "#         return re.findall(emoji_pattern, tweet)\n",
    "    \n",
    "          \n",
    "#     lowercompleteText = remove_hashtags(lowercompleteText)\n",
    "#     lowercompleteText = remove_mentions(lowercompleteText)\n",
    "#     lowercompleteText = remove_numbers(lowercompleteText)\n",
    "#     lowercompleteText = remove_links(lowercompleteText)\n",
    "#     lowercompleteText = remove_emojis(lowercompleteText)\n",
    "#     lowercompleteText = remove_punctuations(lowercompleteText)\n",
    "#   #  lowercompleteText = getstemData(lowercompleteText)\n",
    "#     return  lowercompleteText\n",
    "#     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiments = '\\n'.join(train['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       apple defining language of touch with diffe'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentiments[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "#punc = list(punctuation)\n",
    "\n",
    "# tokenize\n",
    "train['tokenized_text'] = [nltk.word_tokenize(x) for x in train['tweet']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda row: [word for word in row if word not in punc])\n",
    "\n",
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda row: [word for word in row if word not in punc])\n",
    "\n",
    "#train['tweet'] = train['tweet'].apply(lambda row: [word for word in row if word not in punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [apple, defining, language, of, touch, with, d...\n",
       "1    [learning, ab, google, doodles, all, doodles, ...\n",
       "2    [one, of, the, most, in, your, face, ex, of, s...\n",
       "3    [this, iphone, app, would, b, pretty, awesome,...\n",
       "4    [line, outside, the, apple, store, in, waiting...\n",
       "5    [technews, one, lone, dude, awaits, ipad, 2, a...\n",
       "6    [tips, prince, npr, videos, toy, shopping, wit...\n",
       "7    [nu, user, new, ubersocial, for, iphone, now, ...\n",
       "8               [free, sampler, on, itunes, freemusic]\n",
       "9    [i, think, i, might, go, all, weekend, without...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [apple, defining, language, of, touch, with, d...\n",
       "1    [learning, ab, google, doodles, all, doodles, ...\n",
       "2    [one, of, the, most, in, your, face, ex, of, s...\n",
       "3    [this, iphone, app, would, b, pretty, awesome,...\n",
       "4    [line, outside, the, apple, store, in, waiting...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['tokenized_text']\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#tokenized_tweet =train['tokenized_text'].apply(lambda x: x.split()) \n",
    "\n",
    "model_w2v = Word2Vec(train['tokenized_text'], size=200,  window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('temp', 0.9983035922050476),\n",
       " ('shop', 0.9979406595230103),\n",
       " ('congress', 0.9970568418502808),\n",
       " ('in', 0.9963889718055725),\n",
       " ('open', 0.9952287673950195),\n",
       " ('apple', 0.9932007789611816),\n",
       " ('popup', 0.9913644790649414),\n",
       " ('lined', 0.9909828901290894),\n",
       " ('up', 0.9905549883842468),\n",
       " ('set', 0.9903539419174194)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(\"ipad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model_w2v.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['tokenized_text']\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508    [google, maps, for, mobile, 5, 2, looks, awesome]\n",
       "345     [ellen, page, appearance, reduced, to, iphone,...\n",
       "5463    [watching, demo, of, google, hotpot, people, u...\n",
       "5441    [a, delightful, reprieve, from, i, spot, somet...\n",
       "5585    [hi, if, you, accidentally, took, my, ipad, fr...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '''from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized_tweet =X_train.apply(lambda x: x.split()) \n",
    "\n",
    "# model_w2v = Word2Vec(tokenized_tweet, size=200,  window=5)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('circles', 0.9907634258270264),\n",
       " ('denies', 0.987661600112915),\n",
       " ('false', 0.9848948121070862),\n",
       " ('called', 0.9847586154937744),\n",
       " ('tries', 0.9841498136520386),\n",
       " ('possibly', 0.9820309281349182),\n",
       " ('today', 0.9808022975921631),\n",
       " ('announcing', 0.9802318811416626),\n",
       " ('launching', 0.9798178672790527),\n",
       " ('social', 0.9778090715408325)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float64\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model_w2v.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float64\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aniket/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "/Users/Aniket/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 5818\n",
      "Review 2000 of 5818\n",
      "Review 3000 of 5818\n",
      "Review 4000 of 5818\n",
      "Review 5000 of 5818\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "num_features = 200\n",
    "xtrainDataVecs = getAvgFeatureVecs(X_train, model_w2v, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     1\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "      ..\n",
       "170    1\n",
       "171    1\n",
       "172    1\n",
       "173    1\n",
       "174    1\n",
       "175    1\n",
       "176    1\n",
       "177    1\n",
       "178    1\n",
       "179    1\n",
       "180    1\n",
       "181    1\n",
       "182    1\n",
       "183    1\n",
       "184    1\n",
       "185    1\n",
       "186    1\n",
       "187    1\n",
       "188    1\n",
       "189    1\n",
       "190    1\n",
       "191    1\n",
       "192    1\n",
       "193    1\n",
       "194    1\n",
       "195    1\n",
       "196    1\n",
       "197    1\n",
       "198    1\n",
       "199    1\n",
       "Length: 200, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(xtrainDataVecs).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainDataVecs = pd.DataFrame(xtrainDataVecs).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(x.values >= np.finfo(np.float64).max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.train['sentiment'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 50)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest.fit(xtrainDataVecs, y_train)\n",
    "y_pred = forest.predict()\n",
    "classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "170    0\n",
       "171    0\n",
       "172    0\n",
       "173    0\n",
       "174    0\n",
       "175    0\n",
       "176    0\n",
       "177    0\n",
       "178    0\n",
       "179    0\n",
       "180    0\n",
       "181    0\n",
       "182    0\n",
       "183    0\n",
       "184    0\n",
       "185    0\n",
       "186    0\n",
       "187    0\n",
       "188    0\n",
       "189    0\n",
       "190    0\n",
       "191    0\n",
       "192    0\n",
       "193    0\n",
       "194    0\n",
       "195    0\n",
       "196    0\n",
       "197    0\n",
       "198    0\n",
       "199    0\n",
       "Length: 200, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xtrainDataVecs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet'] = test['tweet'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] = [nltk.word_tokenize(x) for x in test['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] = test['tokenized_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet_test =test['tokenized_text'].apply(lambda x: x.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = getAvgFeatureVecs(tokenized_tweet_test,  model_w2v, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"tweet_id\"], \"sentiment\":result})\n",
    "output.to_csv( \"output.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "X = cv.fit_transform(train['tokenized_text']).toarray()\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words ='\\n'.join([str(text) for text in train['tokenized_text']])\n",
    "all_words\n",
    "#all_words = '\\n'.join(train['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# all the tweets\n",
    "\n",
    "# generate wordcloud object\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "# plot wordcloud\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique tokens\n",
    "\n",
    "#unique_tokens = len(set(tok.tokenize(all_sentiments)))\n",
    "\n",
    "#print(f'Unique unprocessed tokens - {unique_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased tokens\n",
    "\n",
    "#unique_lower_case = [t.lower() for t in tok.tokenize(all_sentiments)]\n",
    "#unique_lc_count = len(set(unique_lower_case))\n",
    "\n",
    "#print(f'Unique lower case unprocessed tokens - {unique_lc_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased stemmed tokens\n",
    "\n",
    "#unique_lc_stemmed = [ps.stem(t) for t in unique_lower_case]\n",
    "#unique_lc_stem_count = len(set(unique_lc_stemmed))\n",
    "\n",
    "#print(f'Unique lower case stemmed tokens - {unique_lc_stem_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load stop words\n",
    "\n",
    "#stw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased stemmed w/o stop wordstokens\n",
    "\n",
    "#unique_lc_stem_wo_stpwrds = [t for t in unique_lc_stemmed if t not in stw]\n",
    "#unique_lc_stem_wo_stw_count = len(set(unique_lc_stem_wo_stpwrds))\n",
    "\n",
    "#print(f'Unique lower case stemmed tokens without stop words - {unique_lc_stem_wo_stw_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "\n",
    "#wc = WordCloud(stopwords=stw, background_color='white', max_words=500).generate(all_sentiments.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plt.figure(figsize=(15, 10))\n",
    "#plt.clf()\n",
    "#plt.imshow(wc)\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment categories\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.countplot(train['sentiment'])\n",
    "plt.title('Target Variable Distribution', fontsize=16, color='navy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y\n",
    "#train['tokenized_text'] = [nltk.word_tokenize(x) for x in train['tokenized_text']]\n",
    "\n",
    "#X = train['tokenized_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [apple, defining, language, of, touch, with, d...\n",
       "1    [learning, ab, google, doodles, all, doodles, ...\n",
       "2    [one, of, the, most, in, your, face, ex, of, s...\n",
       "3    [this, iphone, app, would, b, pretty, awesome,...\n",
       "4    [line, outside, the, apple, store, in, waiting...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm=SMOTE(sampling_strategy=0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    59.260278\n",
       "2    32.751272\n",
       "0     6.269765\n",
       "3     1.718686\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [apple, defining, language, of, touch, with, d...\n",
       "1    [learning, ab, google, doodles, all, doodles, ...\n",
       "2    [one, of, the, most, in, your, face, ex, of, s...\n",
       "3    [this, iphone, app, would, b, pretty, awesome,...\n",
       "4    [line, outside, the, apple, store, in, waiting...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in to train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508    [google, maps, for, mobile, 5, 2, looks, awesome]\n",
       "345     [ellen, page, appearance, reduced, to, iphone,...\n",
       "5463    [watching, demo, of, google, hotpot, people, u...\n",
       "5441    [a, delightful, reprieve, from, i, spot, somet...\n",
       "5585    [hi, if, you, accidentally, took, my, ipad, fr...\n",
       "5103    [que, porque, google, to, launch, major, new, ...\n",
       "3800    [yes, gowalla, wins, best, andoid, app, at, th...\n",
       "6093    [quote, of, the, day, on, ipad, interface, des...\n",
       "1402    [the, google, authenticator, app, for, mobile,...\n",
       "6853    [apple, opening, downtown, pop, up, store, for...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_vec = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating count vectorizer from training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate all the models\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression()\n",
    "#gb = GradientBoostingClassifier()\n",
    "xgb= XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert my test data into vectors for prediction\n",
    "\n",
    "#X_test_vec = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-8bcb03dd0d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Logistic Regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xgboost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'CLF report for {name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'M8[ns]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Train the models and check CLF report\n",
    "\n",
    "\n",
    "\n",
    "for model, name in zip([rf, lr, xgb], ['Random Forest', 'Logistic Regression', 'xgboost']):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'CLF report for {name}')\n",
    "    score = metrics.f1_score(y_test, y_pred,average='weighted')\n",
    "    print(score)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier=MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_score=0\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier=MultinomialNB(alpha=alpha)\n",
    "    sub_classifier.fit(X_train,y_train)\n",
    "    y_pred=sub_classifier.predict(X_test)\n",
    "    score = metrics.f1_score(y_test, y_pred,average='weighted')\n",
    "    if score>previous_score:\n",
    "        classifier=sub_classifier\n",
    "    print(\"Alpha: {}, Score : {}\".format(alpha,score))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_tf = tf.fit_transform(X_train)\n",
    "X_test_vec_tf = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models and check CLF report for TFIDF vectorizer\n",
    "\n",
    "for model, name in zip([rf, lr, gb], ['Random Forest', 'Logistic Regression', 'Gradient Boosting']):\n",
    "    model.fit(X_train_vec_tf, y_train)\n",
    "    y_pred = model.predict(X_test_vec_tf)\n",
    "    print(f'CLF report for {name}')\n",
    "    score = metrics.f1_score(y_test,y_pred,average='weighted')\n",
    "    print(\"f1 score:   %0.3f\" % score)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning with RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of unwanted items**\n",
    "1. Hashtags (#)\n",
    "2. Mentions (@)\n",
    "3. Links (http://, https://)\n",
    "4. Short link (t.co, bit.ly)\n",
    "5. Numbers\n",
    "6. Emojis\n",
    "7. Emoticons\n",
    "8. Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
