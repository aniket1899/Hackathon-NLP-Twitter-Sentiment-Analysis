{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "train = pd.read_csv('../data/train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        1\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "tweet        0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ItemID\n",
    "\n",
    "#train.drop('ItemID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           apple defining language of touch with d...\n",
       "1    learning ab google doodles  all doodles should...\n",
       "2    one of the most in your face ex  of stealing t...\n",
       "3    this iphone    app would b pretty awesome if i...\n",
       "4    line outside the apple store in   waiting for ...\n",
       "5     technews one lone dude awaits ipad 2 at apple...\n",
       "6      tips  prince  npr videos  toy shopping with ...\n",
       "7    nu user     new  ubersocial for  iphone now in...\n",
       "8               free    sampler on itunes    freemusic\n",
       "9    i think i might go all weekend without seeing ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiments1 = '\\n'.join(train['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentiments1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hash(text):\n",
    "    hash_pat = r'#'\n",
    "    return re.sub(hash_pat, ' ', text)\n",
    "def remove_mention(text):\n",
    "    mention_pat = r'@mention|@[a-z]+'\n",
    "    return re.sub(mention_pat, ' ', text, flags=re.I)\n",
    "def remove_short_link(text):\n",
    "    short_link_pat = r\"bit\\.ly/[a-z0-9/\\-:\\.=%;,\\+\\*())&\\$!@\\[\\]#\\?~_\\.']*\"\n",
    "    return re.sub(short_link_pat, ' ', text, flags=re.I) \n",
    "def remove_http_link(text):\n",
    "    link_permit = r\"[a-z0-9/\\-:\\.=%;,\\+\\*())&\\$!@\\[\\]#\\?~_\\.']\"\n",
    "    http_link_pat = r\"http[s]?://\"+link_permit+\"+|//\"+link_permit+\"+|[\\w\\.]+\\.[a-z]+/\"+link_permit+\"+\" \n",
    "    return re.sub(http_link_pat, ' ', text, flags=re.I)\n",
    "def remove_sub_link(text):\n",
    "    link_pat = r'{link}'\n",
    "    return re.sub(link_pat, ' ', text, flags=re.I)\n",
    "def remove_html_char(text):\n",
    "    html_char_pat = r'&[a-z]+;'\n",
    "    return re.sub(html_char_pat, ' ', text, flags=re.I)    \n",
    "def remove_date(text):\n",
    "    pipe = r'|'\n",
    "    date_pat_mon = str()\n",
    "    months =   [r'January',\n",
    "                r'February',\n",
    "                r'March',\n",
    "                r'April',\n",
    "                r'May',\n",
    "                r'June',\n",
    "                r'July',\n",
    "                r'August',\n",
    "                r'September',\n",
    "                r'October',\n",
    "                r'November',\n",
    "                r'December']\n",
    "    for month in months:\n",
    "        date_pat_mon = date_pat_mon + month+r' \\d\\d, \\d\\d\\d\\d|'\n",
    "        date_pat_mon = date_pat_mon + month[:3]+r' \\d\\d, \\d\\d\\d\\d|'\n",
    "    date_pat_mon = date_pat_mon[:-1]\n",
    "#     date_pat_mon\n",
    "    date_pat =  r'\\d\\d/\\d\\d/\\d\\d\\d\\d|\\d\\d/\\d\\d/\\d\\d' + pipe + \\\n",
    "                r'\\d\\d\\.\\d\\d\\.\\d\\d\\d\\d|\\d\\d\\.\\d\\d\\.\\d\\d' + pipe + \\\n",
    "                r'\\d\\d-\\d\\d-\\d\\d\\d\\d|\\d\\d-\\d\\d-\\d\\d' + pipe + \\\n",
    "                r'{}'.format(date_pat_mon)\n",
    "    date_pat = r'{}'.format(date_pat)\n",
    "    return re.sub(date_pat, ' ', text, flags=re.I)\n",
    "def remove_short_date(text):\n",
    "    pipe = r'|'\n",
    "    short_date_pat = r'[\\d]?\\d/\\d\\d[\\d\\d]?' + pipe + r'[\\d]?\\d\\.\\d\\d[\\d\\d]?'\n",
    "    short_date_pat = r'{}'.format(short_date_pat)\n",
    "    return re.sub(short_date_pat, ' ', text)\n",
    "def remove_time(text):\n",
    "    pipe = r'|'\n",
    "    time_pat =  r'\\d\\d:\\d\\d:\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d[ ]?am'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d'+pipe+\\\n",
    "                r'\\d\\d:\\d\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d[ ]?am'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d:\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d[ ]?pm'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d[ ]?am'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d'+pipe+\\\n",
    "                r'[\\d]?\\d.\\d\\d'\n",
    "    time_pat= r'{}'.format(time_pat)\n",
    "    return re.sub(time_pat,' ', text, flags=re.I)\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    sxsw_pat = r\"sxsw[a-z]*\"\n",
    "    return re.sub(sxsw_pat, ' ', text, flags=re.I) \n",
    "\n",
    "def remove_austin_texas(text):\n",
    "    austin_pat = r\"austin\"\n",
    "    texas_pat = r\"texas|tx\"\n",
    "    temp = text\n",
    "    temp = re.sub(austin_pat, ' ', temp, flags=re.I) \n",
    "    temp = re.sub(texas_pat, ' ', temp, flags=re.I) \n",
    "    return temp\n",
    "def remove_punctuation(text):\n",
    "    punctuation_pat_s = r'\\'s'\n",
    "    punctuation_pat_t = r'\\'t'\n",
    "    punctuation_pat_d = r'\\'d'\n",
    "    punctuation_pat_ve = r'\\'ve'\n",
    "    punctuation_pat_ll = r'\\'ll'\n",
    "    temp = text\n",
    "    temp = re.sub(punctuation_pat_s, ' ', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_t, 't', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_d, ' would', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_ve, ' have', temp, flags=re.I)\n",
    "    temp = re.sub(punctuation_pat_ll, ' will', temp, flags=re.I)\n",
    "    return temp\n",
    "def remove_rt(text):\n",
    "    rt_pat = r'RT'\n",
    "    return re.sub(rt_pat, ' ', text)\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return re.sub(emoji_pattern,'', text)\n",
    "def remove_not_alnum(text):\n",
    "    \"\"\"\n",
    "    remove everything that is not alpha numeric\n",
    "    \"\"\"\n",
    "    only_text = r'[^a-z0-9 ]'\n",
    "    return re.sub(only_text, ' ', text, flags=re.I)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #replace links witn {link}\n",
    "    text = remove_short_link(text)\n",
    "    text = remove_http_link(text)\n",
    "    #remove {link}\n",
    "    text = remove_sub_link(text)\n",
    "    #hash\n",
    "    text = remove_hash(text)\n",
    "    #mention\n",
    "    text = remove_mention(text)\n",
    "    #rt\n",
    "    text = remove_rt(text)\n",
    "    #html spl chars\n",
    "    text = remove_html_char(text)\n",
    "    #time\n",
    "    text = remove_time(text)\n",
    "    #svxm\n",
    "    text=remove_sxsw(text)\n",
    "    #texas\n",
    "    text=remove_austin_texas(text)\n",
    "    #date\n",
    "    text = remove_date(text)\n",
    "    text = remove_short_date(text)\n",
    "    #punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    #emotjis\n",
    "    text = remove_emojis(text)\n",
    "    #remove spl chars\n",
    "    text = remove_not_alnum(text)\n",
    "    #return lower\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def performCleansingForSentimentalAnalysisWithStopWords(completeText):\n",
    "# #     print('##### Get Tokens in Lower Case #######')\n",
    "# #     #Get tokens in lower case\n",
    "# #     lowercasetokens = getUniqueTokens(completeText.lower())\n",
    "# #     unique_lowercase_tokens = len(set(lowercasetokens))\n",
    "# #     print('Lowercase Unique Tokens are ' + format(unique_lowercase_tokens))\n",
    "# #     print('##### Applying  stemming #######')\n",
    "# #     #Applying the stemming\n",
    "# #     unique_lowercase_tokens_stemmed= stemData(lowercasetokens)\n",
    "# #     len_unique_lowercase_tokens_stemmed = len(unique_lowercase_tokens_stemmed)\n",
    "# #     print('Lowercase Unique Tokens after steming  ' + format(len_unique_lowercase_tokens_stemmed))\n",
    "# #     print('##### Applying stop words + removing Hashtags + mentions + Links + Short Links +  #######')\n",
    "#  #   hashtags = find_hashtags(lowercompleteText)\n",
    "#  #    mentions = find_mentions(lowercompleteText)\n",
    "#  #   links = find_links(lowercompleteText)\n",
    "# #    numbers = find_numbers(lowercompleteText)\n",
    "# #    emojis = find_emojis(lowercompleteText)\n",
    "# #    punctuations = list(punctuation)\n",
    "# #     unique_lowercase_tokens_stemmed_without_stopwords = removeCustomStopWords(lowercompleteText,hashtags,mentions,links,numbers,emojis,punctuations)\n",
    "# #     return unique_lowercase_tokens_stemmed_without_stopwords\n",
    "#     lowercompleteText = completeText.lower()\n",
    "#     import re\n",
    "#     hastag_pat = r'#[0-9a-z_]+'\n",
    "#     mentions_pat = r'\\@[0-9a-z]+'\n",
    "#     number_pat = r'-?\\d+\\.?\\d+|-?\\d+'\n",
    "#     links_pat= r'http://\\S+|https://\\S+'\n",
    "#     punctuations_pat= r'[.!?\\\\-]'\n",
    "#     #short_links_pat =\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     def remove_hashtags(tweet):\n",
    "#         return re.sub(hastag_pat, '', tweet)\n",
    "#     def remove_mentions(tweet):\n",
    "#         return re.sub(mentions_pat, '', tweet)\n",
    "#     def remove_numbers(tweet):\n",
    "#         return re.sub(number_pat, '',tweet)\n",
    "#     def remove_links(tweet):\n",
    "#         return re.sub(links_pat, '', tweet)\n",
    "#     def remove_emojis(tweet):\n",
    "#         return re.sub(emoji_pattern, '', tweet)\n",
    "#     def remove_shortLinks(tweet):\n",
    "#         return re.sub(short_links_pat, '', tweet)\n",
    "#     def remove_punctuations(tweet):\n",
    "#         return re.sub(punctuations_pat, '', tweet)\n",
    "#     def find_hashtags(tweet):\n",
    "#         return re.findall(hastag_pat, tweet)\n",
    "#     def find_mentions(tweet):\n",
    "#         return re.findall(mentions_pat, tweet, flags=re.I)\n",
    "#     def find_links(tweet):\n",
    "#         return re.findall(links_pat, tweet, flags=re.I)\n",
    "#     def find_numbers(tweet):\n",
    "#         return re.findall(number_pat, tweet)\n",
    "#     def find_emojis(tweet):\n",
    "#         return re.findall(emoji_pattern, tweet)\n",
    "    \n",
    "          \n",
    "#     lowercompleteText = remove_hashtags(lowercompleteText)\n",
    "#     lowercompleteText = remove_mentions(lowercompleteText)\n",
    "#     lowercompleteText = remove_numbers(lowercompleteText)\n",
    "#     lowercompleteText = remove_links(lowercompleteText)\n",
    "#     lowercompleteText = remove_emojis(lowercompleteText)\n",
    "#     lowercompleteText = remove_punctuations(lowercompleteText)\n",
    "#   #  lowercompleteText = getstemData(lowercompleteText)\n",
    "#     return  lowercompleteText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           apple defining language of touch with d...\n",
       "1    learning ab google doodles  all doodles should...\n",
       "2    one of the most in your face ex  of stealing t...\n",
       "3    this iphone    app would b pretty awesome if i...\n",
       "4    line outside the apple store in   waiting for ...\n",
       "5     technews one lone dude awaits ipad 2 at apple...\n",
       "6      tips  prince  npr videos  toy shopping with ...\n",
       "7    nu user     new  ubersocial for  iphone now in...\n",
       "8               free    sampler on itunes    freemusic\n",
       "9    i think i might go all weekend without seeing ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiments = '\\n'.join(train['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       apple defining language of touch with diffe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentiments[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "#punc = list(punctuation)\n",
    "\n",
    "# tokenize\n",
    "train['tokenized_text'] = [nltk.word_tokenize(x) for x in train['tweet']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda row: [word for word in row if word not in punc])\n",
    "\n",
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda row: [word for word in row if word not in punc])\n",
    "\n",
    "#train['tweet'] = train['tweet'].apply(lambda row: [word for word in row if word not in punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    apple defining language of touch with differen...\n",
       "1    learning ab google doodles all doodles should ...\n",
       "2    one of the most in your face ex of stealing th...\n",
       "3    this iphone app would b pretty awesome if it d...\n",
       "4    line outside the apple store in waiting for th...\n",
       "5    technews one lone dude awaits ipad 2 at apple ...\n",
       "6    tips prince npr videos toy shopping with zucke...\n",
       "7    nu user new ubersocial for iphone now in the a...\n",
       "8                     free sampler on itunes freemusic\n",
       "9    i think i might go all weekend without seeing ...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#train['tokenized_text'] = train['tokenized_text'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "train['tokenized_text'] = train['tokenized_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    apple defining language of touch with differen...\n",
       "1    learning ab google doodles all doodles should ...\n",
       "2    one of the most in your face ex of stealing th...\n",
       "3    this iphone app would b pretty awesome if it d...\n",
       "4    line outside the apple store in waiting for th...\n",
       "5    technews one lone dude awaits ipad 2 at apple ...\n",
       "6    tips prince npr videos toy shopping with zucke...\n",
       "7    nu user new ubersocial for iphone now in the a...\n",
       "8                     free sampler on itunes freemusic\n",
       "9    i think i might go all weekend without seeing ...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_tweet =train['tokenized_text'].apply(lambda x: x.split()) \n",
    "\n",
    "model_w2v = Word2Vec(tokenized_tweet, size=200,  window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [apple, defining, language, of, touch, with, d...\n",
       "1    [learning, ab, google, doodles, all, doodles, ...\n",
       "2    [one, of, the, most, in, your, face, ex, of, s...\n",
       "3    [this, iphone, app, would, b, pretty, awesome,...\n",
       "4    [line, outside, the, apple, store, in, waiting...\n",
       "5    [technews, one, lone, dude, awaits, ipad, 2, a...\n",
       "6    [tips, prince, npr, videos, toy, shopping, wit...\n",
       "7    [nu, user, new, ubersocial, for, iphone, now, ...\n",
       "8               [free, sampler, on, itunes, freemusic]\n",
       "9    [i, think, i, might, go, all, weekend, without...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('popup', 0.9994679689407349),\n",
       " ('up', 0.999047040939331),\n",
       " ('shop', 0.997204601764679),\n",
       " ('pop', 0.9965310096740723),\n",
       " ('store', 0.9961819052696228),\n",
       " ('line', 0.9960314035415649),\n",
       " ('temp', 0.9941309690475464),\n",
       " ('ipad2', 0.9924941062927246),\n",
       " ('opened', 0.9911863803863525),\n",
       " ('open', 0.9899060726165771)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model_w2v.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aniket/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 7273\n",
      "Review 2000 of 7273\n",
      "Review 3000 of 7273\n",
      "Review 4000 of 7273\n",
      "Review 5000 of 7273\n",
      "Review 6000 of 7273\n",
      "Review 7000 of 7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aniket/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "num_features = 200\n",
    "trainDataVecs = getAvgFeatureVecs(tokenized_tweet, model_w2v, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4564567 , -0.02529931,  0.22828756, ..., -0.3126602 ,\n",
       "        -0.16492665,  0.1218187 ],\n",
       "       [-0.3435992 , -0.07677873,  0.14203027, ..., -0.25336492,\n",
       "        -0.14194435,  0.24446253],\n",
       "       [-0.54228795, -0.03029048,  0.27312258, ..., -0.38668218,\n",
       "        -0.18972911,  0.1317234 ],\n",
       "       ...,\n",
       "       [-0.43778637, -0.09727477,  0.17651181, ..., -0.343147  ,\n",
       "        -0.16840903,  0.28150856],\n",
       "       [-0.48437226, -0.07635547,  0.21842796, ..., -0.3936292 ,\n",
       "        -0.16865684,  0.22171764],\n",
       "       [-0.45458764, -0.0638883 ,  0.2057078 , ..., -0.3417811 ,\n",
       "        -0.16296116,  0.17400773]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trainDataVecs).isnull().sum().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg = XGBClassifier()\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "xgb = xg.fit(trainDataVecs, train[\"sentiment\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet'] = test['tweet'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] = [nltk.word_tokenize(x) for x in test['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] = test['tokenized_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokenized_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet_test =test['tokenized_text'].apply(lambda x: x.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = getAvgFeatureVecs(tokenized_tweet_test,  model_w2v, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"tweet_id\"], \"sentiment\":result})\n",
    "output.to_csv( \"output.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "X = cv.fit_transform(train['tokenized_text']).toarray()\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words ='\\n'.join([str(text) for text in train['tokenized_text']])\n",
    "all_words\n",
    "#all_words = '\\n'.join(train['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# all the tweets\n",
    "\n",
    "# generate wordcloud object\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "# plot wordcloud\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique tokens\n",
    "\n",
    "#unique_tokens = len(set(tok.tokenize(all_sentiments)))\n",
    "\n",
    "#print(f'Unique unprocessed tokens - {unique_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased tokens\n",
    "\n",
    "#unique_lower_case = [t.lower() for t in tok.tokenize(all_sentiments)]\n",
    "#unique_lc_count = len(set(unique_lower_case))\n",
    "\n",
    "#print(f'Unique lower case unprocessed tokens - {unique_lc_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased stemmed tokens\n",
    "\n",
    "#unique_lc_stemmed = [ps.stem(t) for t in unique_lower_case]\n",
    "#unique_lc_stem_count = len(set(unique_lc_stemmed))\n",
    "\n",
    "#print(f'Unique lower case stemmed tokens - {unique_lc_stem_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load stop words\n",
    "\n",
    "#stw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique lower cased stemmed w/o stop wordstokens\n",
    "\n",
    "#unique_lc_stem_wo_stpwrds = [t for t in unique_lc_stemmed if t not in stw]\n",
    "#unique_lc_stem_wo_stw_count = len(set(unique_lc_stem_wo_stpwrds))\n",
    "\n",
    "#print(f'Unique lower case stemmed tokens without stop words - {unique_lc_stem_wo_stw_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "\n",
    "#wc = WordCloud(stopwords=stw, background_color='white', max_words=500).generate(all_sentiments.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plt.figure(figsize=(15, 10))\n",
    "#plt.clf()\n",
    "#plt.imshow(wc)\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment categories\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.countplot(train['sentiment'])\n",
    "plt.title('Target Variable Distribution', fontsize=16, color='navy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y\n",
    "#train['tokenized_text'] = [nltk.word_tokenize(x) for x in train['tokenized_text']]\n",
    "\n",
    "#X = train['tokenized_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm=SMOTE(sampling_strategy=0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in to train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_vec = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating count vectorizer from training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate all the models\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression()\n",
    "#gb = GradientBoostingClassifier()\n",
    "xgb= XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert my test data into vectors for prediction\n",
    "\n",
    "#X_test_vec = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models and check CLF report\n",
    "\n",
    "\n",
    "\n",
    "for model, name in zip([rf, lr, xgb], ['Random Forest', 'Logistic Regression', 'xgboost']):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'CLF report for {name}')\n",
    "    score = metrics.f1_score(y_test, y_pred,average='weighted')\n",
    "    print(score)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier=MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_score=0\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier=MultinomialNB(alpha=alpha)\n",
    "    sub_classifier.fit(X_train,y_train)\n",
    "    y_pred=sub_classifier.predict(X_test)\n",
    "    score = metrics.f1_score(y_test, y_pred,average='weighted')\n",
    "    if score>previous_score:\n",
    "        classifier=sub_classifier\n",
    "    print(\"Alpha: {}, Score : {}\".format(alpha,score))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_tf = tf.fit_transform(X_train)\n",
    "X_test_vec_tf = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models and check CLF report for TFIDF vectorizer\n",
    "\n",
    "for model, name in zip([rf, lr, gb], ['Random Forest', 'Logistic Regression', 'Gradient Boosting']):\n",
    "    model.fit(X_train_vec_tf, y_train)\n",
    "    y_pred = model.predict(X_test_vec_tf)\n",
    "    print(f'CLF report for {name}')\n",
    "    score = metrics.f1_score(y_test,y_pred,average='weighted')\n",
    "    print(\"f1 score:   %0.3f\" % score)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning with RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of unwanted items**\n",
    "1. Hashtags (#)\n",
    "2. Mentions (@)\n",
    "3. Links (http://, https://)\n",
    "4. Short link (t.co, bit.ly)\n",
    "5. Numbers\n",
    "6. Emojis\n",
    "7. Emoticons\n",
    "8. Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
